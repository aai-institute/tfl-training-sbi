{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_sbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "  position: relative;\n",
       "  top: -50%;\n",
       "  margin-left: 5%;\n",
       "}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Conditional Density Estimation</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tfl_training_sbi import mdn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional Density Estimation\n",
    "\n",
    "_\"In statistics, probability density estimation or simply density estimation is the construction of an **estimate, based on observed data**, of an unobservable underlying probability density function.\"_ (Wikipedia)\n",
    "\n",
    "It plays a crucial role in various areas of machine learning, e.g. unsupervised learning, generative modeling, and probabilistic modeling. It allows us to gain insights into the underlying data distribution and enables us to perform various tasks such as outlier detection or data generation.\n",
    "\n",
    "Here we want to use Density Estimation to **approximate a distribution over a free variable, given a set of observations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Density Estimation \n",
    "\n",
    "Given samples $\\theta_i,~i=1,\\dots,n$ with $\\theta_i \\sim p(\\theta)$ but $p(\\theta)$ **unknown**.\n",
    "\n",
    "Goal: find $\\hat{p}(\\theta) \\approx p(\\theta)$.\n",
    "<figure>\n",
    "  <center>\n",
    "  <img src=_static/images/density_est_species.png style=\"width:45%\"/>\n",
    "    <figcaption>Fig. 1 - Plot from <a href\"https://scikit-learn.org/stable/auto_examples/neighbors/plot_species_kde.html\">scikit-learn</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods for Density Estimation\n",
    "\n",
    "- Histograms\n",
    "- Kernel density estimation \n",
    "- Non-parametric approaches \n",
    "- Neural networks (VAEs, GANs, Normalizing Flows, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Given observations $\\theta_i,~i=1,\\dots,n$, find $\\hat{p}(\\theta)$\n",
    "\n",
    "- Assumption: Data follows Normal distribution \n",
    "- Maximize Likelihood of $\\theta_i$ under $\\hat{p}$\n",
    "- Find $\\mu, \\sigma$ where $\\hat{p} := \\mathcal{N}(\\theta_i \\mid \\mu, \\sigma)$ is maximized:\n",
    "\n",
    "$$\n",
    "\\mu^{\\prime}, \\sigma^{\\prime} = \\underset{\\mu, \\sigma}{\\operatorname{argmin}} - \\sum_i^n \\log \\mathcal{N}(\\theta_i; \\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/density_est_toy_example_gauss.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 2 - Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Density Estimation\n",
    "\n",
    "Above, we learned a marginal density $p(\\theta)$. \n",
    "\n",
    "But often, we are interested in conditional density: \n",
    "\n",
    "What is the distribution of New York taxi drop-off locations $x$, given that we picked someone up at $\\theta$? \n",
    "\n",
    "$$\n",
    "p(\\text{drop-off} \\mid \\text{pick-up}=\\theta) = p(x \\mid \\theta)\n",
    "$$\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/taxi.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 3 - Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parametric Conditional Density Estimation\n",
    "\n",
    "Given a fixed value $\\theta$, parameterize a probability density over data $x$.\n",
    "E.g. compute the conditional mean and variance of a Gaussian distribution using a function $f:\\theta \\mapsto \\hat{\\mu}, \\hat{\\sigma}$.\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta) = \\mathcal{N}(x; (\\mu, \\sigma) = f(\\theta))\n",
    "$$\n",
    "\n",
    "$f$ can be any function, e.g. a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estimate the distribution parameters $\\mu(\\theta), \\sigma(\\theta)$ of\n",
    "\n",
    "$$\n",
    "p(x \\mid \\mu(\\theta), \\sigma(\\theta)) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{(x-\\mu(\\theta))^2}{2\\sigma^2(\\theta)})\n",
    "$$\n",
    "\n",
    "by minimizing the negative log-Likelihood of the data under the model.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, \\theta) = - \\sum_i \\log \\mathcal{N}(x_i; \\mu(\\theta_i), \\sigma(\\theta_i))\n",
    "$$\n",
    "\n",
    "Note, this is closely related to Bayesian linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Neural Conditional Density Estimation\n",
    "\n",
    "**Setup**: Observations $x = \\theta + 0.3\\sin(2 \\pi \\theta) + \\epsilon$ with $\\theta\n",
    "\\sim \\mathcal{U}(0,1)$ and $\\epsilon \\sim \\mathcal{N}(0, 0.05)$.\n",
    "\n",
    "**Goal**: Approximate $p(x \\mid \\theta = 0.1)$\n",
    "\n",
    "**Method**: Use a neural network that predicts the parameters of the density model (e.g., a Gaussian). \n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/cde_gaussian.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 4 - Conditional density estimation with parametric Gaussian. Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create  the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 10_000\n",
    "dim = 1\n",
    "\n",
    "theta = torch.rand((sample_size, dim))\n",
    "noise = torch.rand((sample_size, dim)) * 0.2\n",
    "\n",
    "x = theta + 0.3 * torch.sin(2 * math.pi * theta) + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(theta[::15], x[::15], \"go\", alpha=0.2)\n",
    "plt.axvline(0.1, color=\"black\", linestyle=\"--\")\n",
    "plt.ylabel(\"$x$\", fontsize=16)\n",
    "plt.xlabel(\"$\\\\theta$\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Task: Train a neural network to estimate the parameters $\\mu(\\theta), \\sigma(\\theta)$ \n",
    "\n",
    "To do so, you have to create a torch Dataset and DataLoader containing `theta` and `x`. Further, you have to specify a small network with a suitable output dimension and an optimizer. Finally, you have to define the training loop in which the neg. log-Likelihood loss is minimized.\n",
    "\n",
    "You can use the following structure to do so.\n",
    "\n",
    "```python\n",
    "dataset = # todo dataset of theta and x\n",
    "\n",
    "train_loader = # todo: dataloader to the above dataset\n",
    "\n",
    "# define a simple neural network to parameterize the conditional density\n",
    "net = nn.Sequential(\n",
    "    # todo: define model \n",
    ")\n",
    "\n",
    "opt = # todo: define an optimizer \n",
    "\n",
    "# train the neural network for 100 epochs\n",
    "for e in trange(100):\n",
    "    for (\n",
    "        theta_batch,\n",
    "        x_batch,\n",
    "    ) in train_loader:\n",
    "        opt.zero_grad()\n",
    "        nn_output = net(theta_batch)\n",
    "        mean = nn_output[:, 0].unsqueeze(1)\n",
    "        std = torch.exp(nn_output[:, 1]).unsqueeze(1)\n",
    "        prob_Gauss = # todo: implement neg. log-Likelihood loss using assuming a Gaussian distr.\n",
    "        loss = -torch.log(prob_Gauss).sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution task 1.\n",
    "# %load -r 7-36 ./solutions/solutions_nb_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspecting the Learned Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta_cond = torch.tensor([0.1])\n",
    "estimated_params = net(theta_cond)\n",
    "\n",
    "cond_mean, cond_std = (\n",
    "    estimated_params[0].detach().numpy(),\n",
    "    torch.exp(estimated_params[1]).detach().numpy(),\n",
    ")\n",
    "\n",
    "print(f\"Estimated mean: {cond_mean:.3f} and sigma: {cond_std:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(theta[::15], x[::15], \"go\", alpha=0.25)\n",
    "plt.plot(\n",
    "    [theta_cond[0], theta_cond[0]],\n",
    "    [cond_mean - cond_std, cond_mean + cond_std],\n",
    "    \"k-\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.scatter(theta_cond[0], cond_mean, c=\"k\", s=60)\n",
    "\n",
    "plt.ylabel(\"$x$\", fontsize=16)\n",
    "plt.xlabel(\"$\\\\theta$\", fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Problem with this approach\n",
    "\n",
    "- The conditional density estimator might be **too simple** to capture the true distribution\n",
    "- Realistic distributions are more complex than a single Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Test: Switch $\\theta$ and $x$ and check what goes wrong\n",
    "\n",
    "- yields $\\theta = g(x)$\n",
    "- $\\hat{p}(\\theta \\mid x = 0.5) = \\mathcal{N}(\\theta; \\mu(x)=0.44, \\sigma(x)=0.22)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(x[::15], theta[::15], \"go\", alpha=0.25)\n",
    "plt.plot(\n",
    "    [0.5, 0.5],\n",
    "    [0.44 - 0.22, 0.44 + 0.22],\n",
    "    \"k-\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.scatter(0.5, 0.44, c=\"k\", s=60)\n",
    "\n",
    "plt.xlabel(\"$x$\", fontsize=16)\n",
    "plt.ylabel(\"$\\\\theta$\", fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mixture Density Networks\n",
    "\n",
    "Instead of learning to predict the mean and variance of a single Gaussian, a mixture density network (MDN) defines a linear combination of $k$ Gaussians. Each Gaussian distribution can be parameterized. \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= - \\sum_{i=1} \\log q(\\theta_i \\mid x_i) \\\\\n",
    " &= - \\sum_{i=1} \\log \\sum_{j=1}^k \\alpha_j \\mathcal{N}(\\theta_i; \\mu_j(x_i), \\sigma_j(x_i))\n",
    "\\end{align}\n",
    "\n",
    "The MDN learns to predict mixture weights $\\alpha_j$, means $\\mu_j$ and (co-)variances $\\sigma_j$, as a function of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train a Mixture Density Network\n",
    "\n",
    "Use a MDN to parameterize a mixture of Gaussians to approximate the multi-modal distribution of the data shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# prepare data set and data loader\n",
    "# note, the x and theta are swapped compared to the previous example\n",
    "dataset = TensorDataset(x, theta)\n",
    "train_loader = DataLoader(dataset, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Task:** Define a Mixture Density Network using the `MultivariateGaussianMDN` class, provided by the `mdn` module. The object takes four parameters, which you have to define. \n",
    "\n",
    "1. `features`: number of free parameters \n",
    "2. `hidden_net`: a neural network used for feature extraction; this can be `None` but should be constructed in a way that important information can be extracted from the incoming data\n",
    "3. `num_components`: number $k$ from above, i.e. the number of Gaussians in the linear combination \n",
    "4. `hidden_features`: vector size of the output from the feature extractor \n",
    "\n",
    "Feel free to use the following structure. \n",
    "\n",
    "```python\n",
    "mixture_density_net = mdn.MultivariateGaussianMDN(\n",
    "    features= ,         # todo: set the number of variables\n",
    "    hidden_net= ,       # todo: define a feature extractor network\n",
    "    num_components= ,   # todo: define the number of Gaussians \n",
    "    hidden_features= ,  # todo: define the output dimension of the feat. extractor \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution task 2.\n",
    "# %load -r 39-58 ./solutions/solutions_nb_02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit MDN using negative log-likelihood loss\n",
    "opt = Adam(mixture_density_net.parameters(), lr=0.001)\n",
    "num_epochs = 200\n",
    "for e in trange(num_epochs):\n",
    "    for x_batch, theta_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Get log probability of theta under the MDN, given x.\n",
    "        log_probs = mixture_density_net.log_prob(theta_batch, context=x_batch)\n",
    "\n",
    "        # Take negative log prob as loss.\n",
    "        loss = -log_probs.sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspecting the Learned Distribution\n",
    "\n",
    "### Check 1: Sampling\n",
    "\n",
    "Sample a single $\\theta$ for many different conditions $x \\in [0., 1.]$:\n",
    "\n",
    "$$\n",
    "\\theta \\sim \\text{MDN}(\\alpha(x), \\mu(x), \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "x_test = torch.linspace(-0.1, 1.1, 500).unsqueeze(1)\n",
    "\n",
    "for single_x in tqdm(x_test):\n",
    "    # compute the mixture components\n",
    "    weights, means, variances = mixture_density_net.get_mixture_components(\n",
    "        single_x.unsqueeze(1)\n",
    "    )\n",
    "    # sample form mixture of Gaussians\n",
    "    sample = mdn.mog_sample(weights, means, variances)\n",
    "\n",
    "    samples.append(sample)\n",
    "\n",
    "samples = torch.cat(samples).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Check 2: Visualizing the conditional density at a particular $x$\n",
    "\n",
    "We fix one particular $x$, and want to visualize the conditional PDF given that $x$: \n",
    "\n",
    "$$\n",
    "p(\\theta \\mid x = 0.5)\n",
    "$$ \n",
    "\n",
    "To visualize the PDF, we evaluate it along the values $\\theta$ can take, e.g., 100 evenly spaced values between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Task:** Use the trained posterior approximator to compute the conditional distribution over $\\theta$, given an observation, e.g. $x_{\\text{obs}} = 0.5$.\n",
    "\n",
    "Therefore, obtain the parameters for the Gaussian mixture using the `.get_mixture_components(..)` method of the density estimator object defined above. Then, use the `mog_log_prob(..)` function of the `mdn` module (inside a loop) to evaluate the density of all $\\theta_i$ along the axis of $x_{\\text{obs}}$.\n",
    "\n",
    "In order to stick with following cells, please store the obtained densities in a tensor called `demo_probs`.\n",
    "\n",
    "Feel free to use the following structure.\n",
    "\n",
    "```python\n",
    "# chose value to condition on & compute the mixture components\n",
    "x_demo = torch.as_tensor([[..]])  # todo: define the observation\n",
    "logits, means, variances = # todo: obtain mixture parameters \n",
    "\n",
    "demo_probs = []\n",
    "\n",
    "for t_demo in tqdm(torch.linspace(-0.1, 1.0, 100)):\n",
    "    # todo: compute probability of different values of theta for the current parameterization\n",
    "    # ..\n",
    "\n",
    "demo_probs = torch.stack(demo_probs).detach().exp()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution task 3.\n",
    "# %load -r 61-74 ./solutions/solutions_nb_02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "plt.plot(x[::15], theta[::15], \"go\", alpha=0.25, label=\"data\")\n",
    "plt.plot(x_test, samples, \"mo\", alpha=0.5, label=\"samples\")\n",
    "# Note: we scale down the probs only for visualization.\n",
    "plt.plot(probs.numpy() * 0.1 + x_o.numpy(), theta_test, linewidth=2, color=\"black\", \n",
    "         label=r\"$p(\\theta \\mid x=0.5)$\")\n",
    "plt.axvline(x_o.data[0, 0], color=\"black\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"$x$\", fontsize=16)\n",
    "plt.ylabel(\"$\\\\theta$\", fontsize=16)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: Conditional density estimation with MDNs\n",
    "\n",
    "### What are the benefits? \n",
    "\n",
    "- The MDN learns a functional relationship between $x$ and $\\theta$.\n",
    "- This approach can exploit continuities in the parameter space.\n",
    "- Amortization: once trained, the MDN can be used for many different $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are the limitations?\n",
    "\n",
    "- Requires setting of hyperparameters: e.g. components\n",
    "- Training often does not converge perfectly. \n",
    "- In practice, high-dimensional distributions might not be captured perfectly\n",
    "- Normalizing flows, VAEs, and GANs are more flexible. However, Mixtures of Gaussians often allow for useful computations in closed form and are still sometimes used in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "1) What is density estimation? \n",
    "2) What is conditional density estimation? \n",
    "3) What are mixture density networks?\n",
    "4) What is the connection to simulation-based inference? \n",
    "\n",
    "## Anything unclear? Please ask! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgments\n",
    "\n",
    "- Material is based on the [MLColab training](https://mlcolab.org/simulation-based-inference-for-scientific-discovery) on SBI <br/>\n",
    "- Parts of the code from [Mike Dusenberry](https://mikedusenberry.com/mixture-density-networks)  <br/>\n",
    "- Code of MDNs based on Conor Durkan's `lfi` package.  <br/>\n",
    "- Bishop, 1994  <br/>\n",
    "- [MDN graphic](https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca)  \n",
    "- Pedro Gonçalves et al. for figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Thank you for the attention!</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "341.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
