{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_sbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">\n",
    "    <h1>Neural Density Estimation for SBI</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sbi.analysis\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from tfl_training_sbi import mdn\n",
    "from tfl_training_sbi.config import (\n",
    "    default_remote_storage,\n",
    "    get_config,\n",
    "    root_dir,\n",
    ")\n",
    "from tfl_training_sbi.data_utils import (\n",
    "    SIRSimulation,\n",
    "    SIRStdScaler,\n",
    "    load_sir_data,\n",
    ")\n",
    "from tfl_training_sbi.utils_sir import eval_sir_model\n",
    "\n",
    "storage = default_remote_storage()\n",
    "c = get_config(reload=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Neural Density Estimation for SBI\n",
    "\n",
    "- We've seen Likelihood-free inference with ABC\n",
    "- We've seen Conditional Density Estimation\n",
    "\n",
    "Now, we'll combine the idea of Likelihood-free inference with Density Estimation, i.e. Simulation-Based inference (SBI)\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/sbi_concept_figure.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 1 - Schematic overview of SBI. Figure taken from Jan Boelts, 2023</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Density Estimation for SBI\n",
    "\n",
    "Parameterization vs. Learning of a distribution.\n",
    "\n",
    "Methodologies: \n",
    "\n",
    "- Neural Posterior Estimation (NPE) $\\leftarrow$ **This notebook**\n",
    "- Neural Likelihood Estimation (NLE)\n",
    "- Neural Ratio Estimation (NRE)\n",
    "- Neural Score Estimation (NSE)\n",
    "- Flow Matching Posterior Estimation (FMPE)\n",
    "\n",
    "...\n",
    "\n",
    "- And even sequential variants!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea of Neural Posterior Estimation\n",
    "\n",
    "- Learn the posterior $p(\\theta|x)$ of a parameter $\\theta$ given data $x$ using Conditional Density Estimation\n",
    "- How? Use samples from the joint $(\\theta, x) \\sim p(x | \\theta) p(\\theta)$ to learn $p(\\theta|x)$\n",
    "$$\n",
    "p(\\theta \\mid x) \\propto p(x \\mid \\theta) p(\\theta) = p(\\theta, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling from the Joint Distribution\n",
    "\n",
    " 1. Define a prior $p(\\theta)$\n",
    " 2. Define a simulator / generator $g(\\theta)$\n",
    " 3. Draw sample $\\theta \\sim p(\\theta)$ and obtain $x = g(\\theta)$, where $x\\sim p(x\\mid\\theta)$\n",
    "\n",
    "$$\n",
    "p(\\theta, x) = p(\\theta) p(x \\mid \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior decomposition\n",
    "\n",
    "Given the joint distribution visualized in a 2-D plot: \n",
    "\n",
    "- the **likelihood** is a vertical slice through the joint (conditioning on $\\theta$)\n",
    "\n",
    "- the **posterior** is a horizontal slice through the joint (conditioning on $x$)\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/illustration_posterior.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 2 - Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Neural Posterior Estimation with MDNs\n",
    "\n",
    "We will learn the parameterization of Gaussian mixture model using a neural\n",
    "network.\n",
    "\n",
    "Parameters will be adapted using the maximum likelihood principle: minimizing\n",
    "the negative log-likelihood of the data under the model.\n",
    "\n",
    "We will use a Mixture Density Network (MDN) to do so, as seen before. \n",
    "\n",
    "Note: there are more powerful neural density estimators, e.g. Normalizing Flows, or Diffusion Models.\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/illustration_npe_mdn.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 3 - Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example Data: SIR Dataset\n",
    "\n",
    "For this example, the SIR dataset is used. \n",
    "\n",
    "**Task:** Load the data and get familiar with it. Again, a pseudo simulator is\n",
    "provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we provide a pre-computed dataset according to the differential equations\n",
    "# defining the SIR model.\n",
    "# the pseudo simulator draws samples at random\n",
    "sir_theta, sir_x = load_sir_data(c.data)\n",
    "# load class for z-scoring SIR data.\n",
    "sir_scaler = SIRStdScaler()\n",
    "simulator = SIRSimulation(\n",
    "    sir_theta[:-2], sir_x[:-2], transformations=Compose([sir_scaler])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's fix the last pair as observation\n",
    "theta_o_raw, x_o_raw = torch.tensor(sir_theta[-2]), torch.tensor(sir_x[-2])\n",
    "# z-scoring\n",
    "scaled_data = sir_scaler({\"theta\": theta_o_raw, \"x\": x_o_raw})\n",
    "theta_o, x_o = scaled_data[\"theta\"], scaled_data[\"x\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# familiarize yourself with the data; feel free to call this cell several times\n",
    "# as the observation is randomly sampled\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_o_raw, \"o-\", alpha=1.0)\n",
    "plt.title(\"Sample from the simulator, given infection rate and recovery rate\")\n",
    "plt.ylabel(\"Number of infections\")\n",
    "plt.xlabel(\"Sampled time steps\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Task 1:** Define a `torch DataLoader` object which uses the above defined dataset with a suitable batch size. Feel free to experiment with `shuffle` and `drop_last` options. Why and how would you use them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 8-11 ./solutions/solutions_nb_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Task 2:** We're using a mixture of Gaussians to approximate the posterior. The here used `MDN` module allows to provide a `hidden_net` for feature extraction. Define a fully connected feature extractor and initialize a Mixture Density Network as density estimator. Feel free to use the below structure.\n",
    "\n",
    "Furthermore, think about the following parameters. What did you choose?\n",
    "\n",
    "- `features`: number of variable in the problem\n",
    "- `num_components`: number of Gaussians in the mixture \n",
    "- `hidden_features`: output dim. of the linear layer from the feature extractor \n",
    "\n",
    "\n",
    "```python\n",
    "<YOUR-FEAT-EXTRACTOR> = nn.Sequential(\n",
    "    ...,\n",
    ")\n",
    "\n",
    "mixture_density_net = mdn.MultivariateGaussianMDN(\n",
    "    features=<TODO>,\n",
    "    hidden_net=<YOUR-FEAT-EXTRACTOR>,\n",
    "    num_components=<TODO>,\n",
    "    hidden_features=<TODO>,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 14-30 ./solutions/solutions_nb_03.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "num_epochs = 30\n",
    "\n",
    "# fit MDN using negative log-likelihood loss\n",
    "opt = torch.optim.Adam(mixture_density_net.parameters(), lr=0.001)\n",
    "for e in trange(num_epochs):\n",
    "    for batch in sir_dataloader:\n",
    "        theta_batch, x_batch = batch[\"theta\"], batch[\"x\"]\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # get the mixture components\n",
    "        (\n",
    "            weights_of_each_gaussian,\n",
    "            means,\n",
    "            variances,\n",
    "        ) = mixture_density_net.get_mixture_components(x_batch)\n",
    "\n",
    "        # compute Likelihood of sample under current parameterization\n",
    "        log_probs = mdn.mog_log_prob(\n",
    "            theta_batch, weights_of_each_gaussian, means, variances\n",
    "        )\n",
    "\n",
    "        # compute the negative log-likelihood loss\n",
    "        loss = -log_probs.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        # track  the loss per batch\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        # update the parameters using gradient descent\n",
    "        opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Great, you've run through your first SBI workflow! 🎉\n",
    "\n",
    " 1. Define a prior $p(\\theta)$\n",
    " 2. Define a simulator / generator $g(\\theta)$\n",
    " 3. Draw sample $\\theta \\sim p(\\theta)$ and obtain $x = g(\\theta)$, where $x\\sim p(x\\mid\\theta)$\n",
    " 4. Train density estimator $q_{\\phi}(\\theta \\mid x) \\approx p(\\theta \\mid x)$ using simulated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspecting the Approximated Posterior Distribution\n",
    "\n",
    "We have trained a MDN to approximate the posterior distribution over SIR parameters, $q_{\\phi}(\\theta \\mid\n",
    "x_o)$. \n",
    "\n",
    "Below, we will inspect the learned posterior distribution in two steps:\n",
    "\n",
    "**Step 1**. Drawing samples and visualizing the two-dimensional posterior distribution\n",
    "\n",
    "**Step 2**. Simulating data with samples from the posterior and compare to the original observation $x_o$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Task 3:** Use the trained posterior approximator to sample from the conditional distribution, given the test observation. \n",
    "\n",
    "Therefore, obtain the parameters for the Gaussian mixture using the `.get_mixture_components(..)` method of the density estimator object. Then, use the `mog_sample(..)` function of the `mdn` module.\n",
    "\n",
    "In order to stick with following cells, please store the obtained samples in a tensor called `samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 33-45 ./solutions/solutions_nb_03.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# rescale to original parameter ranges.\n",
    "samples_unscaled = sir_scaler.rescale(\n",
    "    {\"theta\": samples, \"x\": torch.ones_like(samples)}\n",
    ")[\"theta\"]\n",
    "\n",
    "# eval. the SIR model for a selected number of samples\n",
    "sample_obs = []\n",
    "subsampling = 16  # we subsample to get 10 time points.\n",
    "for i in trange(1_000):\n",
    "    x = eval_sir_model(theta=samples_unscaled[i])\n",
    "    sample_obs.append(x[::subsampling])  \n",
    "\n",
    "# stack the collected samples into a big array of shape (N, 160, 3)\n",
    "sample_obs = np.stack(sample_obs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Visualizing the Resulting Posterior Approximation\n",
    "\n",
    "Plot the marginal distributions per parameter and the joint distribution of the\n",
    "parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sbi.analysis.pairplot(\n",
    "    [sir_theta[:10000], samples_unscaled[:10000]], points=theta_o_raw, points_offdiag={\"markersize\": 10}, \n",
    "    points_colors=\"r\", limits=[[0.0, 2.5], [0.05, 0.25]], labels=[\"infection rate\", \"recovery rate\"], \n",
    "    offdiag=\"scatter\", legend_kwargs=dict(labels=[\"prior\", \"posterior\", \"theta_o\"]), legend=True, \n",
    "    hist_diag=dict(density=True, bins=\"auto\"),\n",
    "    figsize=(6, 6)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2: Perform a **posterior predictive check**\n",
    "\n",
    "Simulate with parameters sampled from the posterior and compare to observed data $x_o$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval the SIR model for the true parameters to get a full scale observation\n",
    "x_o_full_scale = eval_sir_model(theta=theta_o_raw.numpy())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(sample_obs[:, :, 1].T, \"y--\", alpha=.15)\n",
    "plt.plot([], [], \"y--\", label=\"Simulated from posterior\")\n",
    "plt.plot(x_o_full_scale[::subsampling, 1], \"-\", alpha=1.0, label=\"$x_o$\")\n",
    "\n",
    "plt.xlabel(\"time $t$\")\n",
    "plt.ylabel(\"Proportion of infected people (I)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What if `x` is high-dimensional (image, time series, ...)\n",
    "\n",
    "- Subsampling\n",
    "- Summary Statistics\n",
    "- New: we can learn feature embeddings from data using embedding nets!\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/illustration_mdn_feat_extraction.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 4 - Pre-pending a powerful feature extractor might be necessary in order to obtain meaningful features from time series data. Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential SBI\n",
    "\n",
    "\n",
    "### NPE is fully amortized\n",
    "- NPE is trained on a large range of $x$ (from the prior): \n",
    "    - we can get $p(\\theta \\mid x_o)$ for many different $x_o$\n",
    "- What if we are interested in only one particular $x_o$? \n",
    "\n",
    "\n",
    "### Focusing inference on one $x_o$\n",
    "\n",
    "- Sample inefficiency is a problem of SBI, esp. for costly simulators\n",
    "- Many samples from the prior yield low likelihood / are not informative about $x_o$\n",
    "- **Idea:** re-condition the prior sequentially on $x_o$ to narrow down the \"search space\" --> Sequential Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Sequential Posterior Estimation (SNPE)\n",
    "\n",
    "1. Train a neural density estimator $\\hat{p}_{\\phi}^{(i)}(\\theta \\mid\n",
    "   \\mathbf{x})$ based on $\\theta \\sim p^{(i-1)}(\\theta)$\n",
    "2. Substitute the prior $p^{(i-1)}(\\theta)$ by $\\hat{p}_{\\phi}^{(i)}(\\theta)$\n",
    "3. Train $\\hat{p}_{\\phi}^{(i+1)}(\\theta \\mid \\mathbf{x})$ based on $\\hat{p}_{\\phi}^{(i)}(\\theta)$\n",
    "4. Iterate until convergence\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/illustration_snpe.png\" style=\"width:35%\"/>\n",
    "    <figcaption>Fig. 5 - Illustrating the iterative refinement of the posterior approximation with multi-round inference. Figure obtained JM Lueckmann.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Problem with SNPE\n",
    "\n",
    " - We want to obtain $\\hat{p}(\\theta \\mid \\mathbf{x}) \\approx p(\\theta \\mid \\mathbf{x}) = \\frac{p(x \\mid \\theta) p(\\theta)}{p(x)}$.\n",
    " - We use a proposal prior instead of the real prior, e.g., $\\tilde{p}(\\theta) = q_{\\phi}(\\theta \\mid x)$\n",
    " - When we change the prior to $\\tilde{p}(\\theta)$, we are also changing the posterior.\n",
    "$$\n",
    "    \\tilde{\\hat{p}}(\\theta \\mid \\mathbf{x}) \\approx p(\\mathbf{x} \\mid \\theta) \\tilde{p}(\\theta)\n",
    "$$\n",
    " - Consequence: We need to correct for the change in the prior, to get the actual posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Popular Methods for SNPE\n",
    "\n",
    "All methods introduce a different method to correct the posterior approximation. \n",
    "\n",
    "- SNPE-A (Papamakarios, Murray, 2016, _\"Fast $\\epsilon$-free Bayesian inference...\"_)\n",
    "- SNPE-B (Lueckmann, Goncalves, et al. 2017, _\"Flexible statistical inference...\"_)\n",
    "- SNPE-C (APT) (Greenberg, Nonnenmacher, Macke, 2019, _\"Automatic posterior transformation...\"_)\n",
    "\n",
    "**Note**: Without the sequential approach, all SNPE methods are the same and train by\n",
    "maximizing the log-Likelihood.\n",
    "\n",
    "More recent NPE methods: \n",
    "- FMPE: flow matching neural posterior estimation - density estimation using a continuous normalizing flow (Wildberger, Dax et al., 2023, \"Flow matching for scalable SBI\")\n",
    "- NPSE: neural posterior score estimation - using score-matching diffusion models to approximate the posterior (Sharrock et al. 2022, Geffner et al. 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "#### Neural Posterior Estimation works by\n",
    "\n",
    "  1. Sampling from the prior $\\theta \\sim p(\\theta)$\n",
    "  2. Drawing samples from the joint by forward propagation $\\mathbf{x} \\sim p(\\mathbf{x}|\\theta)$\n",
    "  3. Train a conditional density estimator $\\hat{p}_{\\phi}(\\theta|\\mathbf{x})$\n",
    "on the samples $\\{ (\\theta, \\mathbf{x}) \\}^N_{i=1}$\n",
    "  4. Evaluate the conditional density estimator at certain observations $\\mathbf{x}_o$\n",
    "\n",
    "#### Benefits \n",
    "\n",
    " - NPE is **amortized** s.t. no re-training is required for new observations;\n",
    "   contrary to ABC\n",
    " - (Embedding) Networks allow to cope with high-dimensional data\n",
    "\n",
    "#### Sequential NPE\n",
    "\n",
    " - Enhances **sample efficiency**\n",
    " - Requires adaptations to correct the diverging posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "1) How is neural density estimation used for SBI?\n",
    "2) What is Neural Posterior Estimation?\n",
    "3) What is a posterior predictive check? \n",
    "4) What does amortization mean in the context of SBI?\n",
    "5) What is the motivation for sequential SBI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">\n",
    "    <h1>Thank you for your attention!</h1>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
