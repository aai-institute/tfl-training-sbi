{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_sbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Include title and greeting with divs</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sbi.analysis\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from tfl_training_sbi import mdn\n",
    "from tfl_training_sbi.config import (\n",
    "    default_remote_storage,\n",
    "    get_config,\n",
    "    root_dir,\n",
    ")\n",
    "from tfl_training_sbi.data_utils import (\n",
    "    SIRSimulation,\n",
    "    SIRStdScaler,\n",
    "    load_sir_data,\n",
    ")\n",
    "\n",
    "storage = default_remote_storage()\n",
    "c = get_config(reload=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Density Estimation with SBI\n",
    "\n",
    "- We've seen Likelihood-free inference with ABC\n",
    "- We've seen Density Estimation\n",
    "\n",
    "Now we'll combine the idea of Likelihood-free inference with Density Estimation, i.e. Simulation-based inference (SBI)\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/sbi_concept_figure.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 1 - Schematic overview of SBI. Figure taken from Jan Boelts, 2023</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Density Estimation for SBI\n",
    "\n",
    "Parameterization vs. Learning of a distribution.\n",
    "\n",
    "Methodologies: \n",
    "\n",
    "- Neural Posterior Estimation (NPE) $\\leftarrow$ **This notebook**\n",
    "- Neural Likelihood Estimation (NLE)\n",
    "- Neural Ratio Estimation (NRE)\n",
    "- Neural Score Estimation (NSE)\n",
    "- ...\n",
    "- And even sequential variants!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea of Neural Posterior Estimation\n",
    "\n",
    "- Learn the posterior $p(\\theta|x)$ of a parameter $\\theta$ given data $x$ $\\to$ Conditional Density Estimation\n",
    "- Learn an estimator for $p(\\theta|x)$ on data from the joint distribution $p(\\theta, x)$\n",
    "$$\n",
    "p(\\theta \\mid x) \\propto p(x \\mid \\theta) p(\\theta) = p(\\theta, x)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Joint Distribution\n",
    "\n",
    " 1. Define a prior $p(\\theta)$\n",
    " 2. Define a simulator / generator $g(\\theta)$\n",
    " 3. Draw sample $\\theta \\sim p(\\theta)$ and obtain $x = g(\\theta)$, where $x\\sim p(x\\mid\\theta)$\n",
    "\n",
    "$$\n",
    "p(\\theta, x) = p(\\theta) p(x \\mid \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stick with MDN and hint that there are NFs etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior decomposition\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/illustration_posterior.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 2 - Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Neural Posterior Estimation\n",
    "\n",
    "We will learn the parameterization of Gaussian mixture model using a neural\n",
    "network.\n",
    "\n",
    "Parameters will be adapted using the maximum likelihood principle: minimizing\n",
    "the negative log-likelihood of the data under the model.\n",
    "\n",
    "We will use a Mixture Density Network (MDN) to do so, as seen before. \n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/illustration_npe_mdn.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 3 - Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Data: SIR Dataset\n",
    "\n",
    "For this example, the SIR dataset is used. \n",
    "\n",
    "**Task:** Load the data and get familiar with it. Again, a pseudo simulator is\n",
    "provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we provide a pre-computed dataset according to the differential equations\n",
    "# defining the SIR model.\n",
    "# the pseudo simulator draws samples at random\n",
    "data_theta, data_x = load_sir_data(c.data)\n",
    "sir_scaler = SIRStdScaler()\n",
    "simulator = SIRSimulation(data_theta[:-1], data_x[:-1], transformations=Compose([sir_scaler]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fix the last pair as observation\n",
    "obs_theta, obs_x = torch.tensor(data_theta[-1]).unsqueeze(0), torch.tensor(data_x[-1]).unsqueeze(0)\n",
    "data = sir_scaler({\"theta\": obs_theta, \"obs\": obs_x})\n",
    "obs_theta_z, obs_x_z = data[\"theta\"], data[\"obs\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_x_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: think about inc. the density of samples and plotting everything \n",
    "# familiarize yourself with the data; feel free to call this cell several times\n",
    "# as the observation is randomly sampled\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(simulator()[1][:], \"-\", alpha=0.5)\n",
    "plt.title(\"Sample from the simulator; infection rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: define torch Dataset\n",
    "sir_dataloader = DataLoader(simulator, batch_size=512, shuffle=True, drop_last=True)\n",
    "# todo: preprocessing of data, e.g. standardization or summary stats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the mixture density network, parameterizing a\n",
    "# `num_components`-component mixture of Gaussians with `features`\n",
    "# features, i.e. variables\n",
    "mixture_density_net = mdn.MultivariateGaussianMDN(\n",
    "    features=2,\n",
    "    hidden_net=nn.Sequential(\n",
    "        nn.Linear(10, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "    num_components=4,\n",
    "    hidden_features=16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "\n",
    "# fit MDN using negative log-likelihood loss\n",
    "opt = torch.optim.Adam(mixture_density_net.parameters(), lr=0.001)\n",
    "for e in trange(20):\n",
    "    for batch in sir_dataloader:\n",
    "        theta_batch, x_batch = batch[\"theta\"], batch[\"obs\"]\n",
    "        \n",
    "        opt.zero_grad()\n",
    "\n",
    "        # get the mixture components\n",
    "        (\n",
    "            weights_of_each_gaussian,\n",
    "            means,\n",
    "            variances,\n",
    "        ) = mixture_density_net.get_mixture_components(x_batch)\n",
    "\n",
    "        # compute Likelihood of sample under current parameterization\n",
    "        log_probs = mdn.mog_log_prob(\n",
    "            theta_batch, weights_of_each_gaussian, means, variances\n",
    "        )\n",
    "\n",
    "        # compute the negative log-likelihood loss\n",
    "        loss = -log_probs.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        # track  the loss per batch \n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        # update the parameters using gradient descent\n",
    "        opt.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Learned Distribution\n",
    "\n",
    "Sample `50_000` times from learned posterior distribution $\\hat{p}(\\theta \\mid\n",
    "x_o)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "# get the posterior, i.e. get the mixture components\n",
    "weights, means, variances = mixture_density_net.get_mixture_components(\n",
    "    obs_x_z\n",
    ")\n",
    "\n",
    "for _ in trange(50_000):\n",
    "    # sample form mixture of Gaussians\n",
    "    sample = mdn.mog_sample(weights, means, variances)\n",
    "    samples.append(sample)\n",
    "\n",
    "samples = torch.cat(samples).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sbi.analysis.pairplot(samples, points=obs_theta_z, points_offdiag={\"markersize\": 10}, points_colors=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(samples[:100,:].T, \"--\", alpha=0.1, label=\"samples\")\n",
    "plt.plot(obs_x_z, \"o-\", alpha=1.0, label=\"observed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment of Time Series Data\n",
    "\n",
    "- Subsampling \n",
    "- Summary Statistics\n",
    "- Feature Extraction via different models \n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"_static/images/illustration_mdn_feat_extraction.png\" style=\"width:50%\"/>\n",
    "    <figcaption>Fig. 4 - Pre-pending a powerful feature extractor might be necessary in order to obtain meaningful features from time series data. Figure taken from <a href=\"https://mlcolab.org/simulation-based-inference-for-scientific-discovery\">MLColab</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Neural * Estimation \n",
    "\n",
    "... note that there is such thing but don't go into depth ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Thank you for the attention!</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
