{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_sbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sbi.analysis\n",
    "import sbi.inference\n",
    "import sbi.utils\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# from tfl_training_sbi.config import (\n",
    "#     default_remote_storage,\n",
    "#     get_config,\n",
    "#     root_dir,\n",
    "# )\n",
    "from tfl_training_sbi.data_utils import (\n",
    "    SIRSimulation,\n",
    "    SIRStdScaler,\n",
    "    load_sir_data,\n",
    ")\n",
    "from tfl_training_sbi.utils_sir import eval_sir_model\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "_ = torch.manual_seed(0)\n",
    "\n",
    "# ignore user warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# configure storage location\n",
    "# storage = default_remote_storage()\n",
    "# c = get_config(reload=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Likelihood and Ratio Estimation\n",
    "\n",
    " - Recap of Neural Posterior Estimation \n",
    " - Introduction to Neural Likelihood Estimation (NLE)\n",
    " - Exercise \n",
    " - Neural Ratio Estimation (NRE)\n",
    " - Pros & Cons of all three methods\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: the Goal of SBI\n",
    "\n",
    "- We want to learn the posterior $p(\\theta | \\mathbf{x})$ of a model $\\mathcal{M}$ given some data $\\mathbf{x}$\n",
    "- We do so using Bayes rule: \n",
    "    $$\n",
    "    p(\\theta | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\theta)\n",
    "    p(\\theta)}{p(\\mathbf{x})}\n",
    "    $$\n",
    "- However, the Likelihood is usually intractable\n",
    "- We overcome this challenge by sampling from the joint distribution\n",
    "  $p(\\mathbf{x}, \\theta)=p(\\mathbf{x}\\mid \\theta)p(\\theta)$ and learn a conditional density estimator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Neural Posterior Estimation\n",
    "\n",
    "- Direct mapping from observations $\\mathbf{x}$ to posterior $p(\\theta|\\mathbf{x})$\n",
    "- E.g. learning Gaussian mixture, parameterized by neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Likelihood Function\n",
    "\n",
    "- The Likelihood function is the probability of the data given the model parameters, i.e. $p(\\mathbf{x}|\\theta)$\n",
    "- This is a density function in $\\mathbf{x}$ for fixed $\\theta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning the Likelihood Function with a Neural Network\n",
    "\n",
    "- NLE learns $p(\\mathbf{x}|\\theta)$ instead of $p(\\theta|\\mathbf{x})$\n",
    "- Both are conditional density estimation problems ([NB 02](nb_02_conditional_density_estimation.ipynb))\n",
    "- NLE is a Maximum Likelihood Estimation problem on $\\{(\\theta, \\mathbf{x})_i\n",
    "  \\}^N_{i=1}$ where $\\mathcal{M}(\\theta_i) = \\mathbf{x}_i$\n",
    "- $\\hat{p}(\\theta \\mid \\mathbf{x}) = p(\\theta\\mid\\mathbf{x})p(\\theta)$ is a scaled version with constant $\\frac{1}{p(\\mathbf{x})}$ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling from the NLE Posterior \n",
    "\n",
    "MCMC -> bayes training \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why should I use NLE\n",
    "\n",
    "The goal is to obtain the posterior $p(\\theta \\mid \\mathbf{x})$, why learn\n",
    "$p(\\mathbf{x} \\mid \\theta)$?\n",
    "\n",
    " - Reduced complexity due to factorization for i.i.d. observations \n",
    " - Amortized when increasing the hierarchy \n",
    " - Advantage when $\\operatorname{dim}(\\theta) \\gg \\operatorname{dim}(\\mathbf{x})$\n",
    " - Only the Likelihood is intractable \n",
    " - No correction for sequential learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Intro to the `sbi` Toolkit\n",
    "\n",
    "- Open Source Software for Simulation-Based Inference; <i class=\"fa-brands fa-github\"></i>[GitHub](https://www.mackelab.org/sbi/)\n",
    "- Provides SNPE, SNLE and SNRE as well as analysis tools out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define a uninformative prior\n",
    "prior = sbi.utils.BoxUniform(\n",
    "    low=torch.tensor([0.0, 0.0]), high=torch.tensor([1.0, 1.0]) * 2\n",
    ")\n",
    "\n",
    "\n",
    "# define a simple simulator\n",
    "def example_simulator(theta: torch.tensor):\n",
    "    return torch.sin(theta) + torch.randn_like(theta) * 0.1\n",
    "\n",
    "\n",
    "# obtain samples from joint distribution\n",
    "thetas = prior.sample((1_000,))\n",
    "x = example_simulator(thetas)\n",
    "\n",
    "\n",
    "# use the first sample as observation\n",
    "thetas, theta_obs = thetas[1:, :], thetas[0, :]\n",
    "x, x_obs = x[1:, :], x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# obtain a posterior approx. via NPE\n",
    "inference = sbi.inference.SNPE(prior=prior, density_estimator=\"maf\")\n",
    "density_estimator = inference.append_simulations(thetas, x).train()\n",
    "posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = posterior.sample((100_000,), x=x_obs)\n",
    "_ = sbi.analysis.pairplot(\n",
    "    samples=samples,\n",
    "    points=theta_obs,\n",
    "    points_colors=\"r\",\n",
    "    upper=\"kde\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Application of NLE on the SIR Dataset\n",
    "\n",
    "We will use the same data as in the previous notebook, but now we will use the\n",
    "neural likelihood estimator to construct the posterior.\n",
    "\n",
    "**Task:** Use the provided the pseudo simulator to obtain the SIR dataset. Also,\n",
    "standardized the data and split it into a training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-simulated data from disk\n",
    "data_theta, data_x = load_sir_data(\"./data/\")\n",
    "simulator = SIRSimulation(\n",
    "    data_theta[:-1], data_x[:-1]\n",
    ")\n",
    "\n",
    "# initialize the scaler and the dataset\n",
    "# sir_scaler = SIRStdScaler()\n",
    "# dataset_z = sir_scaler({\"theta\": torch.tensor(data_theta[:-1]), \"obs\": torch.tensor(data_x[:-1])})\n",
    "# theta_z, x_z = dataset_z[\"theta\"], dataset_z[\"obs\"]\n",
    "\n",
    "# remove the last pair from the dataset\n",
    "obs_theta, obs_x = torch.tensor(data_theta[-1]).unsqueeze(0), torch.tensor(\n",
    "    data_x[-1]\n",
    ").unsqueeze(0)\n",
    "\n",
    "# data = sir_scaler({\"theta\": obs_theta, \"obs\": obs_x})\n",
    "# obs_theta_z, obs_x_z = data[\"theta\"], data[\"obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = torch.distributions.LogNormal(\n",
    "    loc=torch.tensor([math.log(0.4), math.log(0.125)]),\n",
    "    scale=torch.tensor([0.5, 0.2]),\n",
    ")\n",
    "prior = sbi.utils.BoxUniform(low=[0,0], high=[1,1])\n",
    "prior.sample((10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, z = data_theta[:1_000, :], data_x[:1_000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a posterior approx. via NPE\n",
    "# todo: the prior is not correct as labels have been scaled\n",
    "inference = sbi.inference.SNLE(prior=prior, density_estimator=\"mdn\")\n",
    "density_estimator = inference.append_simulations(torch.tensor(d), torch.tensor(z)).train(\n",
    "    training_batch_size=128, learning_rate=1e-3, max_num_epochs=5\n",
    ")\n",
    "posterior = inference.build_posterior(density_estimator, sample_with=\"mcmc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Investigate the Learned Posterior \n",
    "\n",
    "In order to investigate the quality of approximation, sample new $\\theta^{\\ast}$ from\n",
    "the prior, evaluate the simulation to obtain $\\mathbf{x}^{\\ast}$ of such and\n",
    "sample from the approximation $\\hat{p}(\\theta \\mid \\mathbf{x}^{\\ast})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use different pairs from the test set\n",
    "obs_theta, obs_x = test_theta[0,:], test_x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_x.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample((100,), x=obs_x)\n",
    "_ = sbi.analysis.pairplot(\n",
    "    samples=samples,\n",
    "    # points=obs_theta,\n",
    "    points_colors=\"r\",\n",
    "    upper=\"kde\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Ratio Estimation \n",
    "\n",
    "- The Likelihood-ratio is a popular test statistic \n",
    "    $$\n",
    "    r(\\mathbf{x} \\mid \\theta_0, \\theta_1) = \\frac{p(\\mathbf{x} \\mid \\theta_0)}{p(\\mathbf{x} \\mid \\theta_1)}\n",
    "    $$\n",
    "- Cranmer et al (2015) have shown that $r(\\mathbf{x}\\mid \\theta_0, \\theta_1)$\n",
    "  can be expressed by a classifier $d(\\mathbf{x}, \\theta)$ trained on\n",
    "  samples from the true $p(\\mathbf{x} \\mid \\theta)$ and an arbitrary but fixed\n",
    "  hypothisis $p(\\mathbf{x} \\mid \\theta_{\\text{ref}})$\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    d(\\mathbf{x}, \\theta) &= p(y=1 \\mid \\mathbf{x}) =\n",
    "    \\frac{p(\\mathbf{x}\\mid \\theta)}{p(\\mathbf{x}\\mid \\theta) + p(\\mathbf{x}\\mid\n",
    "    \\theta_{\\text{ref}})}  \\\\\n",
    "    r(\\mathbf{x} \\mid \\theta_0, \\theta_1) &= \\frac{d(\\mathbf{x},\n",
    "    \\theta)}{ 1 - d(\\mathbf{x}, \\theta) }\n",
    "    \\end{align}\n",
    "    $$\n",
    "  - Thus, learn a classifier to distinguish samples from $p(\\mathbf{x},\n",
    "    \\theta)$ from $p(\\mathbf{x})p(\\theta)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations about NRE\n",
    "\n",
    "- Same concept as NLE but obtaining a normalized posterior \n",
    "- Sampling via MCMC \n",
    "- Training a classifier is easier than training a density estimator\n",
    "- $d(\\mathbf{x}, \\theta)$ will be either $0$ or $1$ for many samples form\n",
    "  $p(\\mathbf{x}, \\theta)$, makeing training more difficult"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of NRE\n",
    "\n",
    "- NRE is also implemented in the `sbi` toolkit\n",
    "\n",
    "    ```python\n",
    "    inference = sbi.inference.SNRE(prior)\n",
    "    density_estimator = inference.append_simulations(thetas, x).train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Considerations for NPE vs. NLE and NRE\n",
    "\n",
    "- NLE and NRE require MCMC; success depends on the shape and dimension of the\n",
    "  posterior \n",
    "- Sequential approaches are helpful for high dimensional problems or costly\n",
    "  simulations. However, SNPE might have issues on sharp boundaries of the prior\n",
    "  and MCMC is slow\n",
    "- Neural networks can take high dimensional inputs easily. Therefore, use NPE\n",
    "  when $\\dim(\\mathbf{x}) \\gg \\dim(\\theta)$ and NLE / NRE when $\\dim(\\mathbf{x})\n",
    "  \\ll \\dim(\\theta)$ \n",
    "- An appropriate feature extractor should be based on the data's structure. E.g.\n",
    "  CNNs for images or RNNs for sequential data.\n",
    "\n",
    "Luckily, SBI and its facets is an active area of research! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "| Method | Pros | Cons |\n",
    "|:---|:---|:---|\n",
    "|NPE | Amortized inference | Sequential version requires correction |\n",
    "|NLE | Likelihood factors for i.i.d. samples, easy usage for hierarchical  problems | Requires MCMC, can be constly in higher dimensions |\n",
    "|NRE | Likelihood factors for i.i.d. samples, training a classifier is easier than training an NDE | Requires MCMC, possible issues for $d$ close to either $0$ or $1$ |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- sbi: A toolkit for simulation-based inference;\n",
    "  [GitHub](https://www.mackelab.org/sbi/credits/)\n",
    "- Cranmer et al. (2015), _\"Approximating Likelihood Ratios with Calibrated Discriminative Classifiers\"_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Thank you for the attention!</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
