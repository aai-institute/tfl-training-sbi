{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_sbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">\n",
    "    <h1>Neural Likelihood and Ratio Estimation</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sbi.analysis\n",
    "import sbi.inference\n",
    "import sbi.utils\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from sbi.utils import process_prior\n",
    "from torch.distributions import LogNormal\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from tfl_training_sbi.config import (\n",
    "    default_remote_storage,\n",
    "    get_config,\n",
    "    root_dir,\n",
    ")\n",
    "from tfl_training_sbi.data_utils import (\n",
    "    SIRSimulation,\n",
    "    load_sir_data,\n",
    ")\n",
    "from tfl_training_sbi.utils_models import save_posterior_obj\n",
    "from tfl_training_sbi.utils_sir import eval_sir_model\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "_ = torch.manual_seed(0)\n",
    "\n",
    "# ignore user warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# configure storage location\n",
    "storage = default_remote_storage()\n",
    "c = get_config(reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    " - Recap of Neural Posterior Estimation \n",
    " - Introduction to Neural Likelihood Estimation (NLE)\n",
    " - Intro to SBI Python package `sbi`\n",
    " - Exercise \n",
    " - Neural Ratio Estimation (NRE)\n",
    " - Pros & Cons of all three methods\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: the Goal of SBI\n",
    "\n",
    "- We want to learn the posterior $p(\\theta | \\mathbf{x})$ of a simulator $\\mathcal{M}$ given some data $\\mathbf{x}$\n",
    "- We do so using Bayes rule: \n",
    "    $$\n",
    "    p(\\theta | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\theta)\n",
    "    p(\\theta)}{p(\\mathbf{x})}\n",
    "    $$\n",
    "- However, the Likelihood is usually intractable\n",
    "- We overcome this challenge by learning a conditional density estimator $q_{\\phi}(\\theta | x)$ from simulated data $(\\theta_i, x_i) \\sim p(x \\mid \\theta)p(\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Neural Posterior Estimation\n",
    "\n",
    "- Direct mapping from observations $\\mathbf{x}$ to posterior $p(\\theta|\\mathbf{x})$\n",
    "- E.g. learning Gaussian mixture, parameterized by neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Likelihood\n",
    "\n",
    "- The Likelihood is the conditional probability density of the data given the model parameters, i.e. $p(\\mathbf{x}|\\theta)$\n",
    "- This is a density function in $\\mathbf{x}$ for fixed $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning the Likelihood with a Neural Network\n",
    "\n",
    "- NLE learns $p(\\mathbf{x}|\\theta)$ instead of $p(\\theta|\\mathbf{x})$\n",
    "- Both are conditional density estimation problems ([NB 02](nb_02_conditional_density_estimation.ipynb))\n",
    "- NLE is a Maximum Likelihood Estimation problem on $\\{(\\theta, \\mathbf{x})_i\n",
    "  \\}^N_{i=1}$ where $\\mathcal{M}(\\theta_i) = \\mathbf{x}_i$\n",
    "- We can obtain posterior samples via $\\hat{p}(\\theta \\mid \\mathbf{x}) \\propto\n",
    "  p(\\mathbf{x} \\mid \\theta)p(\\theta)$, using MCMC or variational inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling from the NLE Posterior \n",
    "\n",
    "- In contrast to NPE, NLE and NRE require MCMC to sample from the posterior\n",
    "- This should be taken into account when deciding which method to use\n",
    "- We refer to the training _Introduction to Bayesian Machine Learning_ for a\n",
    "  more in-depth coverage of accessing the posterior with MCMC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why should I use NLE?\n",
    "\n",
    "The goal is to obtain the posterior $p(\\theta \\mid \\mathbf{x})$, why learn\n",
    "$p(\\mathbf{x} \\mid \\theta)$?\n",
    "\n",
    " - We learn the likelihood for a single $x_i$: efficient for i.i.d. observations\n",
    " - Hierarchical inference: no re-training when I change the hierarchy.\n",
    " - Advantage when $\\operatorname{dim}(\\theta) \\gg \\operatorname{dim}(\\mathbf{x})$\n",
    " - We obtain an emulator: replace expensive simulators\n",
    " - No correction for sequential learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Intro to the `sbi` Toolkit\n",
    "\n",
    "- Open Source Software for SBI on [GitHub](https://sbi-dev.github.io/sbi/)\n",
    "- User-friendly implementations of: \n",
    "    - ABC, \n",
    "    - (S)NPE, (S)NLE, (S)NRE\n",
    "    - MCMC, variational inference\n",
    "    - analysis tools\n",
    "    - plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a uninformative prior\n",
    "num_dims = 2\n",
    "num_simulations = 1000\n",
    "prior = sbi.utils.BoxUniform(\n",
    "    low=torch.zeros(num_dims), high=torch.ones(num_dims) * 2\n",
    ")\n",
    "\n",
    "\n",
    "# define a simple simulator\n",
    "def example_simulator(theta: torch.tensor):\n",
    "    return theta + torch.randn_like(theta) * 0.1\n",
    "\n",
    "\n",
    "# obtain samples from joint distribution\n",
    "thetas = prior.sample((num_simulations,))\n",
    "x = example_simulator(thetas)\n",
    "\n",
    "\n",
    "# use the first sample as observation\n",
    "thetas, theta_obs = thetas[:-2, :], thetas[-2, :]\n",
    "x, x_obs = x[:-2, :], x[-2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# obtain a posterior approx. via NPE\n",
    "# setup methods\n",
    "inference = sbi.inference.SNPE(prior=prior, density_estimator=\"mdn\")  # also supports flows!\n",
    "# run training \n",
    "density_estimator = inference.append_simulations(thetas, x).train(training_batch_size=100)\n",
    "# construct posterior\n",
    "posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "samples = posterior.sample((10_000,), x=x_obs)\n",
    "sbi.analysis.pairplot(samples=samples, points=theta_obs, points_colors=\"r\", \n",
    "                      limits=[[0, 2]]*2, figsize=(6, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Application of NLE on the SIR Dataset\n",
    "\n",
    "We will use the same data as in the previous notebook, but now we will use the\n",
    "neural likelihood estimator to construct the posterior.\n",
    "\n",
    "**Task 1:** Load the pre-simulated SIR data set from disk and split it into a training and test set (a single pair of \"observed\" `(theta_o, x_o)`). \n",
    "\n",
    "Note that `sbi` standardizes the data automatically. Therefore, the scaling step we performed in notebook 3 can be omitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution task 1.\n",
    "# %load -r 6-12 solutions/solutions_nb_04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Task 2:** Furthermore, we have to define a suitable prior to the task. As defined in the benchmarking paper by Lueckmann et al., we'll use a LogNormal distribution with the following parameters:\n",
    " - $\\theta_1 \\sim \\operatorname{LogNormal}(\\log 0.4, 0.5^2)$\n",
    " - $\\theta_2 \\sim \\operatorname{LogNormal}(\\log 0.125, 0.2^2)$\n",
    " \n",
    "Hint: `sbi` has a method `process_prior` (imported above) that takes as input a list of `torch` distributions and combines the entries in the list into a single joint prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution task 2.\n",
    "# %load -r 15-21 solutions/solutions_nb_04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Task 3:** Finally, we can train the NDE. To that end, we have to initialize a SNLE object with the respective prior and method for density estimation. Further, the training data has to be passed to the inference object. The NDE is trained by calling the `.train()` method on the inference object. Feel free to use the above example as blueprint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution task 3.\n",
    "# %load -r 24-30 solutions/solutions_nb_04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Saving the Inference Object \n",
    "\n",
    "The training process of NLE inference objects can take long. Therefore, saving\n",
    "the inference objects is advised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# saving the inference object for possible later use\n",
    "save_posterior_obj(\"../notebooks/models/snle_sir.pl\", inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's Investigate the Learned Posterior \n",
    "\n",
    "In order to investigate the quality of approximation, we'll sample a large\n",
    "number of $\\theta$ from the posterior $p(\\theta \\mid\n",
    "\\mathbf{x}_o)$ using MCMC sampling.\n",
    "\n",
    "Then, we'll compare the resulting distribution to the actual $\\theta^{\\ast}$ that was\n",
    "used to generate $\\mathbf{x}_o$.  \n",
    "\n",
    "**Task 4:** Similar to the example above, use the posterior, conditioned on `x_obs` to sample 1000 $\\theta$s. Then, plot the resulting samples using the `pairplot` method, provided by the `sbi` toolkit. \n",
    "\n",
    "**Note** that because you are using `NLE`, you are effectively using `MCMC` to sample from the posterior (happens by default). To make sampling faster, we recommend passing `method=\"slice_np_vectorized\"` and `num_chains=10` to `sample(...)`. \n",
    "\n",
    "Optional: Use `pairplot` to additionally plot the prior samples as a comparison, e.g., by passing a list of samples `[prior_samples, posterior_sample]`. Use `offdiag=\"scatter\"` option to plot the joint as a scatter plot for better visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution task 4.\n",
    "# %load -r 33-45 solutions/solutions_nb_04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Ratio Estimation \n",
    "\n",
    "- The Likelihood-ratio is a popular test statistic,  \n",
    "\n",
    "    $$\n",
    "    r(\\mathbf{x} \\mid \\theta_0, \\theta_1) = \\frac{p(\\mathbf{x} \\mid \\theta_0)}{p(\\mathbf{x} \\mid \\theta_1)}\n",
    "    $$\n",
    "    \n",
    "- The ratio also appears in the MCMC sampling scheme - having access to it allows us to run MCMC\n",
    "\n",
    "- idea for SBI: approximate the ratio to enable MCMC.\n",
    "\n",
    "- Cranmer et al (2015) showed that $r(\\mathbf{x}\\mid \\theta_0, \\theta_1)$\n",
    "  can be approximated by a classifier $d(\\mathbf{x}, \\theta)$ trained on\n",
    "  samples from the true $p(\\mathbf{x} \\mid \\theta)$ (label $y=1$) and an arbitrary but fixed\n",
    "  hypothesis $p(\\mathbf{x} \\mid \\theta_{\\text{ref}})$ (label $y=0$).\n",
    "\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    d(\\mathbf{x}, \\theta) &= p(y=1 \\mid \\mathbf{x}) =\n",
    "    \\frac{p(\\mathbf{x}\\mid \\theta)}{p(\\mathbf{x}\\mid \\theta) + p(\\mathbf{x}\\mid\n",
    "    \\theta_{\\text{ref}})}  \\\\\n",
    "    r(\\mathbf{x} \\mid \\theta_0, \\theta_1) &= \\frac{d(\\mathbf{x},\n",
    "    \\theta)}{ 1 - d(\\mathbf{x}, \\theta) }\n",
    "    \\end{align}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Considerations about NRE\n",
    "\n",
    "- Same concept as NLE but training a classifier rather than a density estimator (can be easier)\n",
    "- Sampling via MCMC or VI\n",
    "- Allows learning **embeddings for both** $x$ and $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Usage of NRE\n",
    "\n",
    "- NRE is also implemented in the `sbi` toolkit\n",
    "\n",
    "    ```python\n",
    "    inference = sbi.inference.SNRE(prior)\n",
    "    density_estimator = inference.append_simulations(thetas, x).train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Considerations for NPE vs. NLE and NRE\n",
    "\n",
    "- NPE enables fully amortized inference: train once, infer for many different $x_o$ in no-time.\n",
    "- NLE and NRE require MCMC; success depends on the shape and dimension of the posterior \n",
    "- Sequential approaches are helpful for high dimensional problems or costly\n",
    "  simulations. However, SNPE requires a correction term that can make training unstable, SNLE and SNRE require running MCMC for every round.\n",
    "- The type of embeddings that can be learned depend on the method: \n",
    "    - Neural networks can take high dimensional inputs easily. \n",
    "    - NPE can learn embeddings for `x`: helpful when $\\dim(\\mathbf{x}) \\gg \\dim(\\theta)$\n",
    "    - NLE can learn embeddings for `theta`: helpful when $\\dim(\\mathbf{x}) \\ll \\dim(\\theta)$\n",
    "    - NRE can learn embeddings for both \n",
    "- embedding network should take into account the structure of the data. E.g. CNNs for images or RNNs for sequential data.\n",
    "\n",
    "SBI is an active field of research: new methods and improvements are appearing regularly, e.g., score matching or flow matching methods for SBI. \n",
    "\n",
    "`sbi` package is actively developed (and there are other packages too).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "| Method | Pros | Cons |\n",
    "|:---|:---|:---|\n",
    "|NPE | Amortized inference | Sequential version requires correction |\n",
    "|NLE | Likelihood factors for i.i.d. samples, easy usage for hierarchical  problems | Requires MCMC, can be constly in higher dimensions |\n",
    "|NRE | Likelihood factors for i.i.d. samples, training a classifier is easier than training an NDE | Requires MCMC \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- sbi: A toolkit for simulation-based inference;\n",
    "  [GitHub](https://www.mackelab.org/sbi/credits/)\n",
    "- Cranmer et al. (2015), _\"Approximating Likelihood Ratios with Calibrated Discriminative Classifiers\"_\n",
    "- Lueckmann et al. (2021), _\"Benchmarking Simulation-Based Inference\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">\n",
    "    <h2>Thank you for your attention!</h2>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
