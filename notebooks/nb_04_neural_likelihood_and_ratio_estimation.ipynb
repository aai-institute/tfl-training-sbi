{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_sbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sbi.analysis\n",
    "import sbi.inference\n",
    "import sbi.utils\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# from tfl_training_sbi.config import (\n",
    "#     default_remote_storage,\n",
    "#     get_config,\n",
    "#     root_dir,\n",
    "# )\n",
    "from tfl_training_sbi.data_utils import (\n",
    "    SIRSimulation,\n",
    "    SIRStdScaler,\n",
    "    load_sir_data,\n",
    ")\n",
    "from tfl_training_sbi.utils_sir import eval_sir_model\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "_ = torch.manual_seed(0)\n",
    "\n",
    "# ignore user warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# configure storage location\n",
    "# storage = default_remote_storage()\n",
    "# c = get_config(reload=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Likelihood and Ratio Estimation\n",
    "\n",
    " - Recap of Neural Posterior Estimation \n",
    " - Introduction to Neural Likelihood Estimation (NLE)\n",
    " - Exercise \n",
    " - Neural Ratio Estimation (NRE)\n",
    " - Pros & Cons of all three methods\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: the Goal of SBI\n",
    "\n",
    "- We want to learn the posterior $p(\\theta | \\mathbf{x})$ of a model $\\mathcal{M}$ given some data $\\mathbf{x}$\n",
    "- We do so using Bayes rule: \n",
    "    $$\n",
    "    p(\\theta | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\theta)\n",
    "    p(\\theta)}{p(\\mathbf{x})}\n",
    "    $$\n",
    "- However, the Likelihood is usually intractable\n",
    "- We overcome this challenge by sampling from the joint distribution\n",
    "  $p(\\mathbf{x}, \\theta)=p(\\mathbf{x}\\mid \\theta)p(\\theta)$ and learn a conditional density estimator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Neural Posterior Estimation\n",
    "\n",
    "- Direct mapping from observations $\\mathbf{x}$ to posterior $p(\\theta|\\mathbf{x})$\n",
    "- E.g. learning Gaussian mixture, parameterized by neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Likelihood Function\n",
    "\n",
    "- The Likelihood function is the probability of the data given the model parameters, i.e. $p(\\mathbf{x}|\\theta)$\n",
    "- This is a density function in $\\mathbf{x}$ for fixed $\\theta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning the Likelihood Function with a Neural Network\n",
    "\n",
    "- NLE learns $p(\\mathbf{x}|\\theta)$ instead of $p(\\theta|\\mathbf{x})$\n",
    "- Both are conditional density estimation problems ([NB 02](nb_02_conditional_density_estimation.ipynb))\n",
    "- NLE is a Maximum Likelihood Estimation problem on $\\{(\\theta, \\mathbf{x})_i\n",
    "  \\}^N_{i=1}$ where $\\mathcal{M}(\\theta_i) = \\mathbf{x}_i$\n",
    "- $\\hat{p}(\\theta \\mid \\mathbf{x}) = p(\\theta\\mid\\mathbf{x})p(\\theta)$ is a scaled version with constant $\\frac{1}{p(\\mathbf{x})}$ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling from the NLE Posterior \n",
    "\n",
    "- In contrast to NPE, NLE and NRE require MCMC to sample from the posterior\n",
    "- This should be taken into account when deciding which method to use\n",
    "- We refer to the training _Introduction to Bayesian Machine Learning_ for a\n",
    "  more in-depth coverage of accessing the posterior with MCMC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why should I use NLE\n",
    "\n",
    "The goal is to obtain the posterior $p(\\theta \\mid \\mathbf{x})$, why learn\n",
    "$p(\\mathbf{x} \\mid \\theta)$?\n",
    "\n",
    " - Reduced complexity due to factorization for i.i.d. observations \n",
    " - Amortized when increasing the hierarchy \n",
    " - Advantage when $\\operatorname{dim}(\\theta) \\gg \\operatorname{dim}(\\mathbf{x})$\n",
    " - Only the Likelihood is intractable \n",
    " - No correction for sequential learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Intro to the `sbi` Toolkit\n",
    "\n",
    "- Open Source Software for Simulation-Based Inference; <i class=\"fa-brands fa-github\"></i>[GitHub](https://www.mackelab.org/sbi/)\n",
    "- Provides SNPE, SNLE and SNRE as well as analysis tools out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define a uninformative prior\n",
    "prior = sbi.utils.BoxUniform(\n",
    "    low=torch.tensor([0.0, 0.0]), high=torch.tensor([1.0, 1.0]) * 2\n",
    ")\n",
    "\n",
    "\n",
    "# define a simple simulator\n",
    "def example_simulator(theta: torch.tensor):\n",
    "    return torch.sin(theta) + torch.randn_like(theta) * 0.1\n",
    "\n",
    "\n",
    "# obtain samples from joint distribution\n",
    "thetas = prior.sample((1_000,))\n",
    "x = example_simulator(thetas)\n",
    "\n",
    "\n",
    "# use the first sample as observation\n",
    "thetas, theta_obs = thetas[1:, :], thetas[0, :]\n",
    "x, x_obs = x[1:, :], x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# obtain a posterior approx. via NPE\n",
    "inference = sbi.inference.SNPE(prior=prior, density_estimator=\"maf\")\n",
    "density_estimator = inference.append_simulations(thetas, x).train()\n",
    "posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = posterior.sample((100_000,), x=x_obs)\n",
    "_ = sbi.analysis.pairplot(\n",
    "    samples=samples,\n",
    "    points=theta_obs,\n",
    "    points_colors=\"r\",\n",
    "    upper=\"kde\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Application of NLE on the SIR Dataset\n",
    "\n",
    "We will use the same data as in the previous notebook, but now we will use the\n",
    "neural likelihood estimator to construct the posterior.\n",
    "\n",
    "**Task:** Use the provided the pseudo simulator to obtain the SIR dataset. Also,\n",
    "standardized the data and split it into a training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-simulated data from disk\n",
    "# todo: utilize the config here to load the data\n",
    "data_theta, data_x = load_sir_data(\"../data/\")\n",
    "\n",
    "# scale the dataset\n",
    "data_x_z = (data_x - data_x.mean(axis=0)) / data_x.std(axis=0)\n",
    "\n",
    "data_theta, data_x_z = torch.tensor(data_theta_z), torch.tensor(data_x_z)\n",
    "\n",
    "# separate the \"observed\" data from the training data\n",
    "theta_obs, x_obs = data_theta[0,:], data_x_z[0,:]\n",
    "theta_train, x_train = data_theta[1:,:], data_x_z[1:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = torch.distributions.LogNormal(\n",
    "            loc=torch.tensor([math.log(0.4), math.log(0.125)]),\n",
    "            scale=torch.tensor([0.5, 0.2])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 6"
     ]
    }
   ],
   "source": [
    "# obtain a posterior approx. via NPE\n",
    "inference = sbi.inference.SNLE(prior=prior, density_estimator=\"mdn\")\n",
    "density_estimator = inference.append_simulations(theta_train, x_train).train(\n",
    "    training_batch_size=128, learning_rate=1e-3, max_num_epochs=5\n",
    ")\n",
    "posterior = inference.build_posterior(density_estimator, sample_with=\"mcmc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Investigate the Learned Posterior \n",
    "\n",
    "In order to investigate the quality of approximation, we'll sample a large\n",
    "number of $\\theta$ from the conditional density $p(\\theta \\mid\n",
    "\\mathbf{x}_o)$.\n",
    "\n",
    "We'll compare the resulting distribution to the actual $\\theta^{\\ast}$ that was\n",
    "used to generate $\\mathbf{x}_o$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Context `x` needed when a default has not been set.If you'd like to have a default, use the `.set_default_x()` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[39m=\u001b[39m posterior\u001b[39m.\u001b[39;49msample((\u001b[39m1\u001b[39;49m,),)\n\u001b[1;32m      2\u001b[0m _ \u001b[39m=\u001b[39m sbi\u001b[39m.\u001b[39manalysis\u001b[39m.\u001b[39mpairplot(\n\u001b[1;32m      3\u001b[0m     samples\u001b[39m=\u001b[39msamples,\n\u001b[1;32m      4\u001b[0m     \u001b[39m# points=obs_theta,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     points_colors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     upper\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mkde\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/tfl-training-sbi-JNOLu5ZV-py3.9/lib/python3.9/site-packages/sbi/inference/posteriors/mcmc_posterior.py:222\u001b[0m, in \u001b[0;36mMCMCPosterior.sample\u001b[0;34m(self, sample_shape, x, method, thin, warmup_steps, num_chains, init_strategy, init_strategy_parameters, init_strategy_num_candidates, mcmc_parameters, mcmc_method, sample_with, num_workers, show_progress_bars)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m     sample_shape: Shape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mSize(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     show_progress_bars: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, Tuple[Tensor, InferenceData]]:\n\u001b[1;32m    202\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Return samples from posterior distribution $p(\\theta|x)$ with MCMC.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[39m    Check the `__init__()` method for a description of all arguments as well as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m        Samples from posterior.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpotential_fn\u001b[39m.\u001b[39mset_x(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_x_else_default_x(x))\n\u001b[1;32m    224\u001b[0m     \u001b[39m# Replace arguments that were not passed with their default.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m method\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/tfl-training-sbi-JNOLu5ZV-py3.9/lib/python3.9/site-packages/sbi/inference/posteriors/base_posterior.py:141\u001b[0m, in \u001b[0;36mNeuralPosterior._x_else_default_x\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39mreturn\u001b[39;00m process_x(\n\u001b[1;32m    138\u001b[0m         x, x_shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_x_shape, allow_iid_x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpotential_fn\u001b[39m.\u001b[39mallow_iid_x\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mContext `x` needed when a default has not been set.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you\u001b[39m\u001b[39m'\u001b[39m\u001b[39md like to have a default, use the `.set_default_x()` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_x\n",
      "\u001b[0;31mValueError\u001b[0m: Context `x` needed when a default has not been set.If you'd like to have a default, use the `.set_default_x()` method."
     ]
    }
   ],
   "source": [
    "samples = posterior.sample((10_000,), x=x_obs)\n",
    "_ = sbi.analysis.pairplot(\n",
    "    samples=samples,\n",
    "    points=theta_obs,\n",
    "    points_colors=\"r\",\n",
    "    upper=\"kde\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Ratio Estimation \n",
    "\n",
    "- The Likelihood-ratio is a popular test statistic \n",
    "    $$\n",
    "    r(\\mathbf{x} \\mid \\theta_0, \\theta_1) = \\frac{p(\\mathbf{x} \\mid \\theta_0)}{p(\\mathbf{x} \\mid \\theta_1)}\n",
    "    $$\n",
    "- Cranmer et al (2015) have shown that $r(\\mathbf{x}\\mid \\theta_0, \\theta_1)$\n",
    "  can be expressed by a classifier $d(\\mathbf{x}, \\theta)$ trained on\n",
    "  samples from the true $p(\\mathbf{x} \\mid \\theta)$ and an arbitrary but fixed\n",
    "  hypothisis $p(\\mathbf{x} \\mid \\theta_{\\text{ref}})$\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    d(\\mathbf{x}, \\theta) &= p(y=1 \\mid \\mathbf{x}) =\n",
    "    \\frac{p(\\mathbf{x}\\mid \\theta)}{p(\\mathbf{x}\\mid \\theta) + p(\\mathbf{x}\\mid\n",
    "    \\theta_{\\text{ref}})}  \\\\\n",
    "    r(\\mathbf{x} \\mid \\theta_0, \\theta_1) &= \\frac{d(\\mathbf{x},\n",
    "    \\theta)}{ 1 - d(\\mathbf{x}, \\theta) }\n",
    "    \\end{align}\n",
    "    $$\n",
    "  - Thus, learn a classifier to distinguish samples from $p(\\mathbf{x},\n",
    "    \\theta)$ from $p(\\mathbf{x})p(\\theta)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations about NRE\n",
    "\n",
    "- Same concept as NLE but obtaining a normalized posterior \n",
    "- Sampling via MCMC \n",
    "- Training a classifier is easier than training a density estimator\n",
    "- $d(\\mathbf{x}, \\theta)$ will be either $0$ or $1$ for many samples form\n",
    "  $p(\\mathbf{x}, \\theta)$, makeing training more difficult"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of NRE\n",
    "\n",
    "- NRE is also implemented in the `sbi` toolkit\n",
    "\n",
    "    ```python\n",
    "    inference = sbi.inference.SNRE(prior)\n",
    "    density_estimator = inference.append_simulations(thetas, x).train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Considerations for NPE vs. NLE and NRE\n",
    "\n",
    "- NLE and NRE require MCMC; success depends on the shape and dimension of the\n",
    "  posterior \n",
    "- Sequential approaches are helpful for high dimensional problems or costly\n",
    "  simulations. However, SNPE might have issues on sharp boundaries of the prior\n",
    "  and MCMC is slow\n",
    "- Neural networks can take high dimensional inputs easily. Therefore, use NPE\n",
    "  when $\\dim(\\mathbf{x}) \\gg \\dim(\\theta)$ and NLE / NRE when $\\dim(\\mathbf{x})\n",
    "  \\ll \\dim(\\theta)$ \n",
    "- An appropriate feature extractor should be based on the data's structure. E.g.\n",
    "  CNNs for images or RNNs for sequential data.\n",
    "\n",
    "Luckily, SBI and its facets is an active area of research! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "| Method | Pros | Cons |\n",
    "|:---|:---|:---|\n",
    "|NPE | Amortized inference | Sequential version requires correction |\n",
    "|NLE | Likelihood factors for i.i.d. samples, easy usage for hierarchical  problems | Requires MCMC, can be constly in higher dimensions |\n",
    "|NRE | Likelihood factors for i.i.d. samples, training a classifier is easier than training an NDE | Requires MCMC, possible issues for $d$ close to either $0$ or $1$ |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- sbi: A toolkit for simulation-based inference;\n",
    "  [GitHub](https://www.mackelab.org/sbi/credits/)\n",
    "- Cranmer et al. (2015), _\"Approximating Likelihood Ratios with Calibrated Discriminative Classifiers\"_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Thank you for the attention!</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
